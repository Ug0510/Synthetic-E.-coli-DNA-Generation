# -*- coding: utf-8 -*-
"""
Final Year Project: Synthetic Data Generation in Genome Sector (Microbial Chassis)

Description:
This script implements a hybrid GAN model using Biopython and TensorFlow/Keras
to generate synthetic E. coli DNA sequences. It evaluates the sequences for
statistical fidelity (aiming for 95% accuracy conceptually) and biological
plausibility (ORF analysis). A Streamlit interface provides user interaction.

Usage:
1. Install required libraries:
   pip install streamlit tensorflow numpy matplotlib seaborn scikit-learn biopython
2. Run the script from the terminal:
   streamlit run your_script_name.py
"""

# 1. Imports
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from collections import Counter
import io  # To handle BytesIO for downloading

# Biopython imports
from Bio import Entrez, SeqIO
from Bio.Seq import Seq
from Bio.SeqUtils import GC

# TensorFlow / Keras imports
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (
    Dense,
    Reshape,
    Flatten,
    Conv1D,
    Conv1DTranspose,
    BatchNormalization,
    LeakyReLU,
    Input,
    Layer,
)
from tensorflow.keras.optimizers import Adam
import warnings

# Suppress minor warnings
warnings.filterwarnings("ignore")
tf.get_logger().setLevel("ERROR") # Suppress TensorFlow INFO messages

# --- Project Explanation ---
PROJECT_TITLE = "Synthetic Data Generation in Genome Sector (Microbial Chassis)"
PROJECT_EXPLANATION = """
**Objective:** To develop a system for generating realistic synthetic *E. coli* DNA sequences
for applications in microbial chassis engineering, achieving high statistical fidelity (conceptually targeting 95%).

**Methodology:**
1.  **Data Source:** Real *E. coli* K-12 MG1655 genome sequence segments fetched from NCBI.
2.  **Model:** A hybrid Generative Adversarial Network (GAN) built with TensorFlow/Keras.
    *   **Generator:** Learns to create DNA sequences from random noise. Uses transposed convolutions (sometimes called deconvolutions) to upsample noise into sequence representations.
    *   **Discriminator:** Learns to distinguish between real *E. coli* sequences and synthetic ones generated by the Generator. Uses convolutional layers to identify patterns.
    *   **"Hybrid" Aspect:** Refers to the application of modern deep learning (GANs with CNNs) to the specific biological domain of sequence generation, aiming to capture complex patterns beyond simple statistics.
3.  **Training:** Adversarial training process where the Generator tries to fool the Discriminator, and the Discriminator tries to get better at catching fakes. This forces the Generator to produce sequences statistically similar to the real data.
4.  **Evaluation:**
    *   **Quantitative:** Principal Component Analysis (PCA) to visualize if generated sequences cluster near real sequences in a reduced dimensional space. Comparison of GC content and K-mer frequency distributions.
    *   **Qualitative:** Plausibility check (sequence composition) and biological relevance analysis via Open Reading Frame (ORF) identification using Biopython.
5.  **User Interface:** A Streamlit application for generating sequences, viewing analyses, and understanding the results.

**Achieving 95% Fidelity:**
The "95% accuracy" or "fidelity" is a conceptual target representing a high degree of similarity between the statistical properties of generated sequences and real *E. coli* sequences. It's achieved by:
*   **Adversarial Training:** The core mechanism of GANs pushes the generator to mimic the real data distribution accurately to fool the discriminator.
*   **Model Capacity:** Using deep learning models (CNNs) capable of learning complex, long-range dependencies and patterns within the DNA sequences.
*   **Representative Training Data:** Training on a large and diverse dataset of real *E. coli* sequences.
*   **Evaluation Metrics:** Using metrics like K-mer frequencies, GC content distribution, and potentially more advanced sequence similarity measures (beyond the scope of this demo) to quantify the similarity.
*   *Note:* Reaching a provable 95% on all possible statistical measures is extremely challenging. This project demonstrates the methodology and aims for high visual and statistical similarity on key metrics like GC content and K-mer frequencies.

**Application for Microbial Chassis:**
Synthetic DNA sequences with high fidelity are valuable for:
*   **Designing novel gene constructs:** Creating promoters, terminators, or coding sequences with desired properties.
*   **Optimizing metabolic pathways:** Generating sequence variants for enzyme tuning or regulatory element design.
*   **In silico experimentation:** Testing hypotheses about sequence function before expensive lab synthesis.
*   **Data augmentation:** Expanding datasets for training other machine learning models in genomics.
"""

# --- 2. Configuration ---
SEQ_LENGTH = 256  # Length of DNA sequences to generate/process
NOISE_DIM = 100  # Dimension of the random noise input to the Generator
BATCH_SIZE = 32
# NOTE: Reduced epochs significantly for quick demo. Real training needs thousands.
EPOCHS = 50 # Increase for better results, but takes longer
BUFFER_SIZE = 5000  # For shuffling dataset
LEARNING_RATE = 0.0002
KMER_SIZE = 3 # Size of k-mers for frequency analysis
ORF_MIN_LENGTH_AA = 50 # Minimum length of Open Reading Frames in amino acids

# Alphabet for one-hot encoding
ALPHABET = "ACGT"
CHAR_TO_INT = {char: i for i, char in enumerate(ALPHABET)}
INT_TO_CHAR = {i: char for i, char in enumerate(ALPHABET)}
N_CHARS = len(ALPHABET)

# --- 3. Data Loading & Preprocessing ---

# Placeholder for real data - Fetched dynamically
real_sequences_processed = None
real_sequences_raw = []

@st.cache_data(show_spinner="Fetching E. coli K-12 genome segment...")
def load_real_data(accession="U00096.3", max_seqs=5000, seq_len=SEQ_LENGTH):
    """
    Fetches E. coli genome sequence from NCBI and preprocesses it.
    Returns raw sequences and one-hot encoded sequences.
    """
    try:
        Entrez.email = "your.email@example.com" # Provide a dummy email
        handle = Entrez.efetch(db="nucleotide", id=accession, rettype="fasta", retmode="text")
        record = SeqIO.read(handle, "fasta")
        handle.close()

        genome = str(record.seq).upper()
        raw_seqs = []
        processed_data = []

        count = 0
        for i in range(0, len(genome) - seq_len + 1, seq_len // 2): # Overlapping windows
            if count >= max_seqs:
                break
            segment = genome[i : i + seq_len]
            # Basic quality check - ensure only ACGT are present
            if all(c in ALPHABET for c in segment):
                 # One-hot encode
                encoded = np.zeros((seq_len, N_CHARS), dtype=np.float32)
                valid_seq = True
                for j, char in enumerate(segment):
                    if char in CHAR_TO_INT:
                        encoded[j, CHAR_TO_INT[char]] = 1.0
                    else: # Should not happen due to 'all' check, but as safeguard
                        valid_seq = False
                        break
                if valid_seq:
                    processed_data.append(encoded)
                    raw_seqs.append(segment)
                    count += 1

        if not raw_seqs:
             st.error("Could not extract valid sequences. Check accession ID or network.")
             return None, []

        st.success(f"Fetched and processed {len(raw_seqs)} real E. coli sequences.")
        return np.array(processed_data), raw_seqs
    except Exception as e:
        st.error(f"Error fetching/processing data from NCBI: {e}")
        st.info("Using fallback placeholder data.")
        # Simple fallback placeholder if NCBI fails
        placeholder_seq = "A"*seq_len
        encoded = np.zeros((seq_len, N_CHARS), dtype=np.float32)
        encoded[:, 0] = 1.0 # All 'A's
        return np.array([encoded] * 10), [placeholder_seq] * 10


def sequences_to_onehot(sequences):
    """Converts a list of DNA strings to one-hot encoded numpy array."""
    data = np.zeros((len(sequences), SEQ_LENGTH, N_CHARS), dtype=np.float32)
    for i, seq in enumerate(sequences):
        if len(seq) != SEQ_LENGTH:
            st.warning(f"Sequence length mismatch: {len(seq)} vs {SEQ_LENGTH}. Skipping.")
            continue
        for j, char in enumerate(seq):
            if char in CHAR_TO_INT:
                data[i, j, CHAR_TO_INT[char]] = 1.0
            # Ignore characters not in ALPHABET (e.g., N) implicitly
    return data

def onehot_to_sequences(onehot_data):
    """Converts one-hot encoded numpy array back to DNA strings."""
    sequences = []
    for i in range(onehot_data.shape[0]):
        seq = ""
        # Ensure probabilities sum to 1 or handle potential floating point issues
        indices = np.argmax(onehot_data[i], axis=1)
        for index in indices:
             # Map integer index back to character
             seq += INT_TO_CHAR.get(index, 'N') # Use 'N' for invalid indices
        sequences.append(seq)
    return sequences


# --- 4. GAN Model ---

# Custom layer to convert continuous Generator output to one-hot probabilities
class GumbelSoftmax(Layer):
    def __init__(self, temperature=0.5, **kwargs):
        super(GumbelSoftmax, self).__init__(**kwargs)
        self.temperature = temperature

    def call(self, inputs):
        # Add Gumbel noise
        gumbel_noise = -tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(inputs), minval=0, maxval=1) + 1e-9) + 1e-9)
        # Apply softmax with temperature (Gumbel-Softmax trick)
        y_soft = tf.nn.softmax((inputs + gumbel_noise) / self.temperature, axis=-1)
        # In testing phase, use hard argmax, but backprop through soft version
        y_hard = tf.one_hot(tf.argmax(y_soft, axis=-1), depth=tf.shape(inputs)[-1], dtype=inputs.dtype)
        # Straight-through estimator
        y = tf.stop_gradient(y_hard - y_soft) + y_soft
        return y

def build_generator(noise_dim=NOISE_DIM, output_len=SEQ_LENGTH, n_chars=N_CHARS):
    model = Sequential(name="Generator")
    model.add(Input(shape=(noise_dim,)))
    # Project noise to a higher dimension space suitable for convolutions
    model.add(Dense(8 * 8 * 128)) # Adjusted starting size
    model.add(Reshape((8, 8 * 128))) # Initial reshape dimensions
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))

    # Upsample using Conv1DTranspose
    model.add(Conv1DTranspose(128, kernel_size=4, strides=2, padding='same')) # Output shape: (None, 16, 128)
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv1DTranspose(64, kernel_size=4, strides=2, padding='same')) # Output shape: (None, 32, 64)
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv1DTranspose(32, kernel_size=4, strides=2, padding='same')) # Output shape: (None, 64, 32)
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv1DTranspose(16, kernel_size=4, strides=2, padding='same')) # Output shape: (None, 128, 16)
    model.add(BatchNormalization())
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv1DTranspose(n_chars, kernel_size=4, strides=2, padding='same')) # Output shape: (None, 256, n_chars)

    # Ensure output shape matches SEQ_LENGTH
    # If output length isn't exactly SEQ_LENGTH, adjust layers or add a final Dense/Reshape if needed.
    # This architecture should output (None, 256, 4) if SEQ_LENGTH is 256

    # Use GumbelSoftmax to get one-hot like discrete output for DNA bases
    model.add(GumbelSoftmax(temperature=0.5)) # Temperature can be tuned

    # Ensure final shape is exactly (SEQ_LENGTH, N_CHARS)
    # This might require padding/cropping if strides don't perfectly match, or adjust kernel/stride sizes
    # For this example, we assume the ConvTranspose layers result in the correct length 256

    return model

def build_discriminator(input_len=SEQ_LENGTH, n_chars=N_CHARS):
    model = Sequential(name="Discriminator")
    model.add(Input(shape=(input_len, n_chars)))

    model.add(Conv1D(32, kernel_size=5, strides=2, padding="same")) # Output shape: (None, 128, 32)
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv1D(64, kernel_size=5, strides=2, padding="same")) # Output shape: (None, 64, 64)
    # model.add(BatchNormalization()) # Optional BN
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv1D(128, kernel_size=5, strides=2, padding="same")) # Output shape: (None, 32, 128)
    # model.add(BatchNormalization()) # Optional BN
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv1D(256, kernel_size=5, strides=2, padding="same")) # Output shape: (None, 16, 256)
    # model.add(BatchNormalization()) # Optional BN
    model.add(LeakyReLU(alpha=0.2))

    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid')) # Output: probability (real/fake)

    return model

# --- 5. Training Logic ---
# Define loss and optimizers
cross_entropy = tf.keras.losses.BinaryCrossentropy()
generator_optimizer = Adam(learning_rate=LEARNING_RATE, beta_1=0.5)
discriminator_optimizer = Adam(learning_rate=LEARNING_RATE, beta_1=0.5)

# Define loss functions
def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

# Define single training step (compiled using tf.function for speed)
# @tf.function # Temporarily disable tf.function for easier debugging if needed
def train_step(images, generator, discriminator):
    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)

        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

    return gen_loss, disc_loss

def train_gan(dataset, epochs, generator, discriminator, steps_per_epoch):
    """Trains the GAN for a specified number of epochs."""
    history = {'gen_loss': [], 'disc_loss': []}
    progress_bar = st.progress(0)
    status_text = st.empty()

    total_steps = epochs * steps_per_epoch
    current_step = 0

    for epoch in range(epochs):
        epoch_gen_loss = []
        epoch_disc_loss = []
        status_text.text(f"Epoch {epoch + 1}/{epochs}")

        step_in_epoch = 0
        for image_batch in dataset:
            if step_in_epoch >= steps_per_epoch:
                 break # Limit steps per epoch for faster demo cycles

            gen_loss, disc_loss = train_step(image_batch, generator, discriminator)
            epoch_gen_loss.append(gen_loss.numpy())
            epoch_disc_loss.append(disc_loss.numpy())

            current_step += 1
            step_in_epoch += 1
            progress_bar.progress(current_step / total_steps)


        avg_gen_loss = np.mean(epoch_gen_loss)
        avg_disc_loss = np.mean(epoch_disc_loss)
        history['gen_loss'].append(avg_gen_loss)
        history['disc_loss'].append(avg_disc_loss)

        status_text.text(f"Epoch {epoch + 1}/{epochs} - Gen Loss: {avg_gen_loss:.4f}, Disc Loss: {avg_disc_loss:.4f}")

    progress_bar.empty() # Clear progress bar on completion
    status_text.success("Training finished!")
    return history

# Function to generate sequences using the trained generator
def generate_synthetic_sequences(generator, num_sequences):
    noise = tf.random.normal([num_sequences, NOISE_DIM])
    generated_onehot = generator(noise, training=False) # Use training=False for inference
    # Convert the output (potentially soft probabilities) to hard one-hot vectors
    # generated_onehot_hard = tf.one_hot(tf.argmax(generated_onehot, axis=-1), depth=N_CHARS)
    # generated_sequences = onehot_to_sequences(generated_onehot_hard.numpy())
    generated_sequences = onehot_to_sequences(generated_onehot.numpy()) # GumbelSoftmax might already be sharp enough
    return generated_sequences


# --- 6. Sequence Analysis Functions ---

def calculate_gc_content(sequences):
    """Calculates GC content for a list of sequences."""
    gc_contents = []
    for seq_str in sequences:
        if len(seq_str) > 0:
            try:
                gc_contents.append(GC(Seq(seq_str)))
            except Exception as e:
                 # Handle potential errors with non-standard characters if any slip through
                 st.warning(f"Could not calculate GC for sequence: {seq_str[:20]}... Error: {e}")
                 gc_contents.append(np.nan) # Append NaN for problematic sequences
        else:
            gc_contents.append(np.nan)
    return [gc for gc in gc_contents if not np.isnan(gc)] # Filter out NaN values


def find_orfs(sequences, min_len_aa=ORF_MIN_LENGTH_AA):
    """Finds Open Reading Frames (ORFs) in sequences."""
    all_orfs_info = []
    for i, seq_str in enumerate(sequences):
        seq = Seq(seq_str)
        orfs_found = []
        # Check all 6 frames (3 forward, 3 reverse complement)
        for strand, nuc in [(+1, seq), (-1, seq.reverse_complement())]:
            for frame in range(3):
                length = len(nuc[frame:])
                # Ensure length is multiple of 3 for translation
                translatable_seq = nuc[frame : frame + (length // 3) * 3]
                if not translatable_seq: continue

                try:
                    # Using standard bacterial code (NCBI table 11)
                    # Stop codons are '*' in Biopython translation
                    # We look for Methionine ('M') starts
                    protein_seq = translatable_seq.translate(table=11) # Stop codons included as '*'
                    aa_sequences = str(protein_seq).split('*') # Split by stop codon

                    current_pos_aa = 0
                    for aa_seq in aa_sequences[:-1]: # Process segments ending with '*'
                        start_codon_indices = [j for j, aa in enumerate(aa_seq) if aa == 'M']
                        for start_index in start_codon_indices:
                            orf_aa = aa_seq[start_index:]
                            if len(orf_aa) >= min_len_aa:
                                # Calculate original DNA coordinates
                                start_dna = frame + (current_pos_aa + start_index) * 3
                                end_dna = start_dna + len(orf_aa) * 3 + 3 # Include stop codon

                                # Adjust coordinates for reverse strand
                                if strand == -1:
                                    orig_start = len(seq) - end_dna
                                    orig_end = len(seq) - start_dna
                                else:
                                    orig_start = start_dna
                                    orig_end = end_dna

                                orfs_found.append({
                                    "Sequence #": i + 1,
                                    "Frame": frame + 1 if strand == 1 else -(frame + 1),
                                    "Start (DNA)": orig_start,
                                    "End (DNA)": orig_end,
                                    "Length (AA)": len(orf_aa),
                                    "Protein Seq (start)": orf_aa[:15] + ("..." if len(orf_aa) > 15 else "")
                                })
                        current_pos_aa += len(aa_seq) + 1 # Add 1 for the '*' separator

                except Exception as e:
                    # Catch potential translation errors (e.g., ambiguous bases)
                    # st.warning(f"Translation error in sequence {i+1}, frame {frame}, strand {strand}: {e}")
                    pass # Silently ignore errors for this demo

        all_orfs_info.extend(orfs_found)
    return all_orfs_info


def calculate_kmer_freqs(sequences, k=KMER_SIZE):
    """Calculates k-mer frequencies for a list of sequences."""
    all_kmers = Counter()
    total_kmers = 0
    for seq in sequences:
        if len(seq) >= k:
            for i in range(len(seq) - k + 1):
                kmer = seq[i:i+k]
                # Only count valid k-mers
                if all(c in ALPHABET for c in kmer):
                    all_kmers[kmer] += 1
                    total_kmers += 1

    # Normalize frequencies
    freqs = {kmer: count / total_kmers for kmer, count in all_kmers.items()} if total_kmers > 0 else {}
    return freqs


# --- 7. Evaluation Functions ---

def plot_gc_distribution(real_seqs, generated_seqs, ax):
    """Plots GC content distribution for real and generated sequences."""
    gc_real = calculate_gc_content(real_seqs)
    gc_generated = calculate_gc_content(generated_seqs)

    if not gc_real or not gc_generated:
        st.warning("Not enough valid sequences to plot GC distribution.")
        ax.text(0.5, 0.5, "Insufficient Data for GC Plot", ha='center', va='center')
        return

    sns.histplot(gc_real, color="blue", label=f'Real E. coli (n={len(gc_real)})', kde=True, ax=ax, stat="density", bins=20)
    sns.histplot(gc_generated, color="red", label=f'Generated (n={len(gc_generated)})', kde=True, ax=ax, stat="density", bins=20)
    ax.set_title('GC Content Distribution')
    ax.set_xlabel('GC Content (%)')
    ax.set_ylabel('Density')
    ax.legend()

def plot_kmer_frequencies(real_seqs, generated_seqs, k=KMER_SIZE, top_n=30, ax=None):
    """Plots and compares k-mer frequencies."""
    freqs_real = calculate_kmer_freqs(real_seqs, k)
    freqs_generated = calculate_kmer_freqs(generated_seqs, k)

    if not freqs_real or not freqs_generated:
        st.warning(f"Not enough valid sequences to plot {k}-mer distribution.")
        if ax: ax.text(0.5, 0.5, f"Insufficient Data for {k}-mer Plot", ha='center', va='center')
        return None # Indicate failure

    # Get all unique k-mers present in either set
    all_kmers_union = sorted(list(set(freqs_real.keys()) | set(freqs_generated.keys())))

    # For clarity, plot only the top N most frequent k-mers from the real data
    # Or alternatively, plot all if the number is manageable
    kmers_to_plot = sorted(freqs_real, key=freqs_real.get, reverse=True)[:top_n]
    if len(all_kmers_union) <= top_n * 1.5: # If total kmers isn't much larger than top_n
        kmers_to_plot = all_kmers_union # Plot all
    else:
         # Ensure we also include top generated kmers if they differ significantly
         top_generated = sorted(freqs_generated, key=freqs_generated.get, reverse=True)[:top_n]
         kmers_to_plot = sorted(list(set(kmers_to_plot) | set(top_generated)))


    real_vals = [freqs_real.get(kmer, 0) for kmer in kmers_to_plot]
    gen_vals = [freqs_generated.get(kmer, 0) for kmer in kmers_to_plot]

    if ax: # If an axis is provided, plot on it
        df = pd.DataFrame({'K-mer': kmers_to_plot, 'Real': real_vals, 'Generated': gen_vals})
        df.plot(x='K-mer', y=['Real', 'Generated'], kind='bar', ax=ax, figsize=(12, 6))
        ax.set_title(f'Top {len(kmers_to_plot)} {k}-mer Frequencies')
        ax.set_ylabel('Frequency')
        ax.tick_params(axis='x', rotation=90)
        ax.grid(axis='y', linestyle='--')
    else: # Otherwise, return data for other uses (like distance calculation)
         # Calculate a simple distance metric (e.g., Jensen-Shannon Divergence or Euclidean distance)
         # For simplicity here, just return the raw frequencies
         return freqs_real, freqs_generated

    return True # Indicate success


def plot_pca_comparison(real_data_onehot, generated_data_onehot, ax):
    """Performs PCA and plots the comparison."""
    if real_data_onehot is None or generated_data_onehot is None or \
       real_data_onehot.shape[0] < 2 or generated_data_onehot.shape[0] < 2:
        st.warning("Not enough data points for PCA visualization.")
        ax.text(0.5, 0.5, "Insufficient Data for PCA", ha='center', va='center')
        return

    # Flatten the one-hot encoded data for PCA (N_samples x (Seq_len * N_chars))
    real_flat = real_data_onehot.reshape(real_data_onehot.shape[0], -1)
    gen_flat = generated_data_onehot.reshape(generated_data_onehot.shape[0], -1)

    # Combine data for PCA fitting
    combined_data = np.vstack((real_flat, gen_flat))
    labels = ['Real'] * len(real_flat) + ['Generated'] * len(gen_flat)

    if combined_data.shape[0] < 2 or combined_data.shape[1] < 2:
        st.warning("PCA requires at least 2 samples and 2 features after flattening.")
        ax.text(0.5, 0.5, "Insufficient Dimensions for PCA", ha='center', va='center')
        return

    pca = PCA(n_components=2)
    try:
        principal_components = pca.fit_transform(combined_data)
    except Exception as e:
        st.error(f"PCA failed: {e}")
        st.warning(f"Data shape: {combined_data.shape}")
        # Add debug info: check for NaNs or Infs
        if np.isnan(combined_data).any(): st.warning("PCA input contains NaNs.")
        if np.isinf(combined_data).any(): st.warning("PCA input contains Infs.")
        ax.text(0.5, 0.5, "PCA Error", ha='center', va='center')
        return


    pc_df = pd.DataFrame(data=principal_components, columns=['PC 1', 'PC 2'])
    pc_df['Source'] = labels

    sns.scatterplot(data=pc_df, x='PC 1', y='PC 2', hue='Source', alpha=0.7, ax=ax, palette={'Real': 'blue', 'Generated': 'red'})
    ax.set_title('PCA of Real vs Generated Sequences')
    ax.set_xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')
    ax.set_ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')
    ax.legend()
    ax.grid(True)

# --- 8. Streamlit UI ---
st.set_page_config(layout="wide", page_title=PROJECT_TITLE)
st.title(PROJECT_TITLE)
st.markdown("A demonstration of generating synthetic *E. coli* DNA using GANs.")

# Sidebar for controls and info
st.sidebar.header("Controls")
# Load Data Button - Placed early
if st.sidebar.button("Load/Fetch E. coli Data"):
    # Fetch real data (cached)
    real_sequences_processed, real_sequences_raw = load_real_data(seq_len=SEQ_LENGTH)
    if real_sequences_processed is not None:
        st.session_state['real_data_processed'] = real_sequences_processed
        st.session_state['real_data_raw'] = real_sequences_raw
        st.sidebar.success(f"Loaded {len(real_sequences_raw)} real sequences.")
    else:
        st.sidebar.error("Failed to load real data.")

# Check if data is loaded before proceeding
if 'real_data_processed' not in st.session_state:
    st.warning("Please load the *E. coli* data using the button in the sidebar first.")
    st.stop() # Stop execution until data is loaded

# Display Project Explanation
with st.expander("Project Details and Explanation", expanded=False):
    st.markdown(PROJECT_EXPLANATION)

# --- GAN Training Section ---
st.header("1. GAN Training (Demonstration)")
st.markdown(f"""
Below, you can initiate a *brief* training process for the GAN.
The Generator learns to produce {SEQ_LENGTH}bp sequences resembling *E. coli* DNA.
**Note:** Effective GAN training requires **many more epochs** (thousands) and potentially more data than this demo uses.
This section primarily demonstrates the training *mechanism*.
""")

col1, col2 = st.columns([1, 3])
with col1:
    num_epochs_demo = st.slider("Training Epochs (Demo)", min_value=1, max_value=100, value=EPOCHS, help="Number of training loops. Higher is better but slower.")
    train_button = st.button("Start Demo Training")

with col2:
    st.markdown("**Training Status:**")
    status_placeholder = st.empty() # Placeholder for status messages/plots
    loss_plot_placeholder = st.empty() # Placeholder for the loss plot

# Initialize models in session state if not present
if 'generator' not in st.session_state:
    st.session_state['generator'] = build_generator(noise_dim=NOISE_DIM, output_len=SEQ_LENGTH, n_chars=N_CHARS)
    st.session_state['discriminator'] = build_discriminator(input_len=SEQ_LENGTH, n_chars=N_CHARS)
    st.session_state['training_history'] = None

if train_button:
    status_placeholder.info("Preparing data and starting training...")
    # Prepare dataset for training
    real_data_tf = tf.data.Dataset.from_tensor_slices(st.session_state['real_data_processed'])
    real_data_tf = real_data_tf.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

    # Calculate steps per epoch based on dataset size and batch size
    num_real_samples = st.session_state['real_data_processed'].shape[0]
    steps_per_epoch = min(num_real_samples // BATCH_SIZE, 100) # Limit steps/epoch for speed
    if steps_per_epoch == 0:
        st.error("Not enough real data samples for the selected batch size. Cannot train.")
        st.stop()


    # Create or get existing models
    generator = st.session_state['generator']
    discriminator = st.session_state['discriminator']

    # Train
    with st.spinner(f"Training GAN for {num_epochs_demo} epochs..."):
        history = train_gan(real_data_tf, num_epochs_demo, generator, discriminator, steps_per_epoch)
    st.session_state['training_history'] = history
    st.session_state['generator'] = generator # Save trained generator
    status_placeholder.success("Demo training complete!")

    # Plot training loss
    if st.session_state['training_history']:
        fig_loss, ax_loss = plt.subplots()
        ax_loss.plot(history['gen_loss'], label='Generator Loss')
        ax_loss.plot(history['disc_loss'], label='Discriminator Loss')
        ax_loss.set_title('GAN Training Loss (Demo)')
        ax_loss.set_xlabel('Epoch')
        ax_loss.set_ylabel('Loss')
        ax_loss.legend()
        loss_plot_placeholder.pyplot(fig_loss)


# --- Sequence Generation and Analysis ---
st.header("2. Generate & Analyze Synthetic Sequences")

num_sequences_to_gen = st.number_input("Number of sequences to generate:", min_value=1, max_value=200, value=10)
generate_button = st.button("Generate Synthetic Sequences")

if 'generated_sequences_raw' not in st.session_state:
    st.session_state['generated_sequences_raw'] = []
    st.session_state['generated_sequences_onehot'] = None
    st.session_state['orf_results'] = None

if generate_button:
    if 'generator' not in st.session_state:
        st.error("GAN model (Generator) not available. Please run the demo training first or load a pre-trained model (if implemented).")
    else:
        with st.spinner("Generating sequences with the trained Generator..."):
            generator = st.session_state['generator']
            generated_sequences = generate_synthetic_sequences(generator, num_sequences_to_gen)
            st.session_state['generated_sequences_raw'] = generated_sequences
            # Also generate one-hot versions for PCA
            st.session_state['generated_sequences_onehot'] = sequences_to_onehot(generated_sequences)


if st.session_state['generated_sequences_raw']:
    st.subheader("Generated Sequences (Examples)")
    # Display first few sequences
    display_area = st.text_area("Generated DNA:",
                                "\n".join([f">Seq_{i+1}\n{seq}" for i, seq in enumerate(st.session_state['generated_sequences_raw'][:20])]),
                                height=200)

    # Add download button for generated sequences
    sequences_str = "\n".join([f">Generated_Seq_{i+1}\n{seq}" for i, seq in enumerate(st.session_state['generated_sequences_raw'])])
    sequences_bytes = sequences_str.encode('utf-8')
    st.download_button(
        label="Download Generated Sequences (FASTA)",
        data=sequences_bytes,
        file_name="generated_ecoli_sequences.fasta",
        mime="text/plain"
    )


    st.subheader("Analysis of Generated Sequences")
    # Perform analyses
    generated_gc = calculate_gc_content(st.session_state['generated_sequences_raw'])
    if generated_gc:
         st.metric("Average GC Content (Generated)", f"{np.mean(generated_gc):.2f}%")
    else:
        st.info("Could not calculate GC content for generated sequences.")


    st.markdown("**Open Reading Frame (ORF) Analysis:**")
    st.markdown(f"Searching for ORFs with minimum length: {ORF_MIN_LENGTH_AA} amino acids.")
    with st.spinner("Finding ORFs..."):
         orf_results = find_orfs(st.session_state['generated_sequences_raw'], min_len_aa=ORF_MIN_LENGTH_AA)
         st.session_state['orf_results'] = orf_results # Save results

    if st.session_state['orf_results']:
        st.write(f"Found {len(st.session_state['orf_results'])} potential ORFs matching criteria.")
        # Display ORF results in a table (using pandas if available, otherwise simple text)
        try:
            import pandas as pd
            orf_df = pd.DataFrame(st.session_state['orf_results'])
            st.dataframe(orf_df.head(10)) # Show first 10 ORFs found
            # Add download button for ORF results
            csv = orf_df.to_csv(index=False).encode('utf-8')
            st.download_button(
                 label="Download ORF Results (CSV)",
                 data=csv,
                 file_name='generated_orf_results.csv',
                 mime='text/csv',
             )
        except ImportError:
            st.write("Install pandas (`pip install pandas`) for better table display.")
            # Fallback display
            for orf in st.session_state['orf_results'][:10]:
                 st.text(f"Seq {orf['Sequence #']}, Frame {orf['Frame']}, Pos {orf['Start (DNA)']}-{orf['End (DNA)']}, Len {orf['Length (AA)']} AA")
    else:
        st.info("No ORFs meeting the minimum length criteria were found in the generated sequences.")


# --- Evaluation Section ---
st.header("3. Model Evaluation")
st.markdown("Comparing statistical properties of generated sequences against real *E. coli* data.")

# Ensure we have both real and generated data for comparison
if 'real_data_raw' in st.session_state and st.session_state['generated_sequences_raw']:
    real_seqs_eval = st.session_state['real_data_raw']
    gen_seqs_eval = st.session_state['generated_sequences_raw']

    # Limit number of real sequences used for plotting if very large
    max_real_for_plot = 1000
    if len(real_seqs_eval) > max_real_for_plot:
        # Randomly sample real sequences for fairer visual comparison density
        indices = np.random.choice(len(real_seqs_eval), max_real_for_plot, replace=False)
        real_seqs_eval_plot = [real_seqs_eval[i] for i in indices]
        real_data_processed_plot = st.session_state['real_data_processed'][indices]
    else:
        real_seqs_eval_plot = real_seqs_eval
        real_data_processed_plot = st.session_state['real_data_processed']


    # Create plots
    fig_eval, axes = plt.subplots(1, 3, figsize=(20, 6)) # Use 1 row, 3 cols layout

    # Plot 1: GC Content Distribution
    plot_gc_distribution(real_seqs_eval_plot, gen_seqs_eval, ax=axes[0])

    # Plot 2: K-mer Frequencies
    plot_kmer_frequencies(real_seqs_eval_plot, gen_seqs_eval, k=KMER_SIZE, ax=axes[1])

    # Plot 3: PCA Comparison
    # Ensure generated one-hot data is available
    if st.session_state['generated_sequences_onehot'] is not None:
         plot_pca_comparison(real_data_processed_plot, st.session_state['generated_sequences_onehot'], ax=axes[2])
    else:
         axes[2].text(0.5, 0.5, "Generate sequences first for PCA", ha='center', va='center')
         axes[2].set_title('PCA Comparison')

    plt.tight_layout()
    st.pyplot(fig_eval)

else:
    st.info("Generate synthetic sequences (Section 2) after loading real data (Sidebar) to see evaluation results.")


# --- Sample Output Section (as comments/text) ---
st.header("4. Sample Output & Interpretation")
st.markdown(f"""
*   **Generated Sequences:** The text area above shows examples of the {SEQ_LENGTH}bp DNA sequences created by the GAN. Check for plausible composition (mostly A, C, G, T).
*   **GC Content:** The average GC content of generated sequences should ideally be close to that of real *E. coli* (around 50.8%). The distribution plot compares the spread.
*   **ORF Analysis:** The table shows potential protein-coding regions found within the generated sequences. The presence of reasonably long ORFs suggests biological potential. Comparing the length distribution of these ORFs to those in real *E. coli* segments would be a further validation step.
*   **K-mer Frequencies:** This plot compares the frequency of short motifs (like 'ACG', 'TTG', etc.). A good generator should replicate the k-mer usage patterns of real *E. coli*. Significant divergence indicates the model hasn't fully captured the sequence statistics.
*   **PCA Plot:** This visualization reduces the high-dimensional sequence data into 2 dimensions. Ideally, the red dots (Generated) should overlap significantly with the blue dots (Real), indicating the generator is producing sequences within the same data manifold as the real sequences. Clear separation suggests the generated sequences are statistically different.

**Note on 95% Fidelity:** Achieving and proving 95% fidelity requires rigorous statistical testing across multiple metrics (e.g., comparing distributions of k-mers of various lengths, codon usage bias, motif occurrences) and likely much more extensive training than shown in this demo. The visualizations provided here offer a qualitative and quantitative glimpse into the model's performance relative to this goal.
""")

# --- Footer ---
st.sidebar.markdown("---")
st.sidebar.markdown("Developed for a Final Year Project.")


# --- Main execution logic ---
# The Streamlit app structure implicitly handles the flow.
# Data loading is triggered by a button.
# Training is triggered by a button.
# Generation is triggered by a button, which then enables analysis and evaluation display.
